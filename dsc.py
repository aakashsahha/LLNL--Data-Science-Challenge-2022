# -*- coding: utf-8 -*-
"""DSC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KeAzTYHM4aB93gWks9DSSCyB38Uy0G9X
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import xgboost as xgb
import lightgbm as lgb

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import (ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier)
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import plot_roc_curve, roc_auc_score, roc_curve
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score

# simple example of how to read the data files

# read task 1 data
data_csv = "DSC_2022/gitlab/data/mpro_exp_data2_rdkit_feat.csv"
data_df = pd.read_csv(data_csv)

data_df.drop(columns= ['lib_name','feat_10','feat_41','feat_62','feat_68','feat_81','feat_85','feat_108','feat_118','feat_124','feat_125','feat_126','feat_127','feat_128','feat_130','feat_131','feat_132'
    ,'feat_136','feat_137','feat_138','feat_141','feat_142','feat_143','feat_144','feat_145','feat_146','feat_147','feat_148','feat_152','feat_155','feat_156','feat_157','feat_159','feat_161'
    ,'feat_162','feat_163','feat_166','feat_167','feat_169','feat_170','feat_171','feat_172','feat_173','feat_174','feat_175','feat_176','feat_177','feat_178','feat_180','feat_181','feat_182'
    ,'feat_183','feat_184','feat_185','feat_186','feat_187','feat_189','feat_190','feat_191','feat_192','feat_194','feat_195','feat_196','feat_198','feat_199','feat_200','feat_201','feat_202'
    ,'feat_203','feat_204','feat_208'], inplace=True)
data_df.replace([np.inf, -np.inf], np.nan, inplace=True)
data_df.dropna(inplace=True)





for column in data_df:
    if data_df[column].dtypes == 'float64':
        if (data_df[column].quantile(0.75) - (data_df[column].quantile(0.25))) == 0:
            data_df.drop(columns=[column],inplace=True)
        else:
            data_df[column] = (data_df[column] - data_df[column].median())/(data_df[column].quantile(0.75) - data_df[column].quantile(0.25))


# Check if the dataset is imbalanced
print(data_df.shape)
print(print(data_df.groupby(['label','subset']).size()))


"""Task 1:"""

df_test = data_df[data_df['subset'] == 'test']
df_train = data_df[data_df['subset'] == 'train']
df_valid = data_df[data_df['subset'] == 'valid']

df_train = pd.concat([df_train,df_valid])
"""Training:"""

# The train matrix consist of 208 feature vectors
X_train = df_train[df_train.columns[5:]]
X_test = df_test[df_test.columns[5:]]
#X_valid = df_valid[df_test.columns[5:]]


Y_train = df_train[df_train.columns[3]]
Y_test = df_test[df_test.columns[3]]
#Y_valid = df_valid[df_valid.columns[3]]


"""Some hyperparameter tuning 

What is a hyperparameter: Any parameter that is external to the model and cannot be estimated from data.

A very nice article: https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac


"""


gbc = GradientBoostingClassifier()
grf = RandomForestClassifier()
xg = xgb.XGBClassifier()
parameters = {
    "n_estimators":[500,600,700,800,900,1000],
    "max_depth":[3,4,5,7,9],
    "learning_rate":[0.1,0.2,0.3,0.4,0.5]
}

cv = GridSearchCV(xg,parameters,cv=5, scoring = 'precision', n_jobs = 5, verbose=0)
cv.fit(X_train,Y_train)

def display(results):
    print(f'Best parameters are: {results.best_params_}')
    print("\n")
    mean_score = results.cv_results_['mean_test_score']
    std_score = results.cv_results_['std_test_score']
    params = results.cv_results_['params']
    for mean,std,params in zip(mean_score,std_score,params):
        print(f'{round(mean,3)} + or -{round(std,3)} for the {params}')


display(cv)

clf = LogisticRegression(random_state=0).fit(X_train, Y_train)
#clf_gb = GradientBoostingClassifier(n_estimators=700,learning_rate=0.2,max_depth=9, verbose=1,random_state=1).fit(X_train,Y_train)
#print(clf_gb.score(X_test,Y_test))
print(clf.score(X_test,Y_test))

#clf_rf = RandomForestClassifier(n_estimators=700, max_features ='auto', max_depth=9, verbose=1,random_state=1).fit(X_train,Y_train)
#print(clf_rf.score(X_test,Y_test))



plt.figure()

# Add the models to the list that you want to view on the ROC plot
models = [
{
    'label': 'Random Forest',
    'model': RandomForestClassifier(n_estimators=700, max_features ='auto', max_depth=9, verbose=1,random_state=1),
},
{
    'label': 'Gradient Boosting',
    'model': GradientBoostingClassifier(n_estimators=800,learning_rate=0.2,max_depth=9, verbose=1,random_state=1),
},
{
    'label': 'XGBoost',
    'model': xgb.XGBClassifier(n_estimators=600,learning_rate=0.2,max_depth=5, verbose=1,random_state=1),
}
]

# Below for loop iterates through your models list
for m in models:
    model = m['model'] # select the model
    model.fit(X_train, Y_train) # train the model
    Y_pred = model.predict(X_test) # predict the test data
    # Compute False postive rate, and True positive rate
    fpr, tpr, thresholds = roc_curve(Y_test, model.predict_proba(X_test)[:,1])
    print("Accuracy :", model.score(X_test,Y_test))
    print("Precision :", precision_score(Y_test,Y_pred))
    print("Recall :", recall_score(Y_test,Y_pred))
    print("f1 score :", f1_score(Y_test,Y_pred))
    # Calculate Area under the curve to display on the plot
    auc = roc_auc_score(Y_test,model.predict(X_test))
    # Now, plot the computed values
    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (m['label'], auc))

# Custom settings for the plot 
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('1-Specificity (False Positive Rate)')
plt.ylabel('Sensitivity (True Positive Rate)')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()   # Display


